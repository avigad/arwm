Notes
-----

This directory contains notes on the use of Isabelle's automation to prove a simple theorem. My hope is that the data will provide insight as to what automation can and cannot do in the context of interactive theorem proving.

Suppose we are given a set of dominos such that each domino covers two adjacent squares on ordinary 8 x 8 chessboard. Remove two opposite corners. The *mutilated chessboard* problem asks whether it is possible to cover the remaining squares with dominos. The answer is no: the two squares that have been removed have the same color, and every domino covers one black square and one white square.

The files [mutilated_ideal.thy](mutilated_ideal.thy) and [mutilated_ideal.pdf](mutilated_ideal.pdf) contain an Isar outline of the proof, at the level one would write a careful and detailed informal proof. If automation could fill in every `sorry`, we could declare victory. Students learning to do mathematics can be taught to write proofs at that level of detail. At the other end of the spectrum, a mathematician who comes up with a subtle argument and wants confirmation that it is correct could be expected to write it down in that fashion. My goal was then to use fill out the templates using automation and take detailed notes along the way, to get a sense of how far we are from that ideal.

The rules I set for myself are as follows. I wanted to prove the theorem only by adding intermediate statements and relying on Isabelle's basic automation, `auto`, `simp`, and Sledgehammer. I allowed myself to browse theory files online to figure out how to *state* things, e.g. to discover that `bij_betw f s t` says that `f` is a bijection between sets `s` and `t`. Of course, knowing basic notation and identifiers like this were necessary even to write down the original proof sketch. But I wanted to avoid looking for particular theorems or using other tactics. I have been away from Isabelle for a while, and the details of the library, the proof language, and the tactics are not fresh in my mind. So I did not really have to pretend to be a user who wanted to avoid dealing with those details.

Here is some more background: `auto` and `simp` represent internal automation, and Sledgehammer calls battery of external provers and then uses the results to construct a proof. I used the default: resolution provers E, Spass, and Vampire, and SMT solvers CVC4 and Z3. I made use of Isar's mechanisms to manage the context. Intermediate statements can be labeled, phrases like `from ... have`, `hence`, and `with` control which previous statements appear in the local goal. (Keywords `hence` and `with` are syntactic sugar for `from this have` and `from this ... have`, where `this` refers to the previous goal.) Additionally, one can tell `auto` and `simp` to use additional facts with expressions like `auto simp add: ....` or `auto add: ...`. The first adds the fact for use by the simplifier, and the second adds it to the tableau search. The keyword `using` before `auto` or `simp` simply adds the facts to the hypotheses in the goal.

The files [mutilated_actual.thy](mutilated_actual.thy) and [mutilated_actual.pdf](mutilated_actual.pdf) are the proofs that I came up with. They are not meant to be pretty. The tendency among formalizers upon completing a proof like this is to clean it up and shorten it to leave only a record of what *should* have been done. Instead, I tried to leave an accurate record of what I actually *did*, to better measure the level of difficulty. I even left the senseless numbering scheme for auxiliary hypotheses. Whenever I needed a label, I made one up, without concern for uniformity or structure.

Finally, and I hope most usefully, the file [mutilated_notes.md](mutilated_notes.md) provides a detailed play-by-play account of my mucking around. My hope is to provide developers with information as to what an ordinary user might do, and what the pain points are.

Towards the goal of making automation useful for the formalization of mathematics, my assessment is that we are making progress but we still have a long way to go. If I were working with Isabelle regularly, I would devote time to relearning the details of the library and the tactics, and my guess is that my use of Sledgehammer would be limited. I expect that I would use particular theorems to perform the main steps, call `auto` and `simp` to dispel easy subgoals, and call Sledgehammer once in a while when I thought it might save me the trouble of looking through the library for a particular fact. For comparison, I recently formalized mutilated chessboard problem in Lean, relying only on the simplifier. (The formalization is available here: http://www.andrew.cmu.edu/user/avigad/Papers/mutilated.pdf.) Searching for low-level facts and chaining them together in precise ways is also tedious, and it would be nice to avoid that. But the use of Sledgehammer in this example did not make the task less onerous, and I am hoping we can do better.

It is possible that I am not using Sledgehammer well, and that seasoned Sledgehammer users will be better at filling out the template. In that case, I would love to see not only the resulting proofs, but a play-by-play account of how they are obtained. In interactive theorem proving, we learn a lot by watching others. If there are heuristics or a workflow that makes the use fo Sledgehammer more effective, it would be helpful to document it.

It seems that the main problem with the use of Sledgehammer is that it is an all-or-nothing affair: when it fails, users have no recourse except to guess at the reasons that it failed and blindly throw extra information at it. As Jasmin Blanchette has pointed out to me, it would be helpful if Sledgehammer could report near misses to the user. For example, unit clauses or short clauses at the point of failure suggest hypotheses that are sufficient to prove the goal. But, as Jasmin points out, the nature of superposition calculi is that the search may get stuck on a clause that could otherwise be shortened, thereby failing to identify the real sticking points. And I don't know whether this method could be used to suggest candidate equations as well.

The tools `auto` and `simp` do much better in this respect, since they do whatever they can and leave the resulting goals for the user to inspect. Squinting at complicated goals is not fun, but it often makes it clear what has gone wrong and what additional information is needed. It seems to me that the main problem with these tools is that they work best when the search space is limited, that is, basically when the solution boils down to doing the obvious things. When they work, they work extremely well, but when a goal requires a bit of creativity -- picking a term instantiation, or relating one concept to another in a manner that amounts to more than unpacking a definition -- the tools fall short.

In any event, I will try to prepare a few more templates before the London talk, to provide further examples of straightforward mathematical inferences that I would like to see automated. It would be good if others did the same, since this might provide researchers in the field with helpful benchmarks and targets.
